
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="keywords" content="computer vision, machine learning, ML datasets and benchmark, continual learning, image recognition, online continual learning"/>
  <title>The CLEAR Benchmark: Continual LEArning on Real-World Imagery
</title>

  <link href="css/normalize.css" rel="stylesheet" type="text/css"/>
  <link href="css/reset.css" rel="stylesheet" type="text/css"/>
  <link href="css/style.css" rel="stylesheet" type="text/css"/>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

</head>
<body>
<div class="container">
  <div class="project-page">

    <div class="affiliation">
      <a href="https://www.ri.cmu.edu/"><img class="leftalign" src="img/cmu_03.png"></a>
    </div>

    <hr>
    <h1>
      <i>The CLEAR Benchmark:</i><br> Continual LEArning on Real-World Imagery</br>
    </h1>

    <hr>

    <div class="authors">
      <a href="https://linzhiqiu.github.io/">Zhiqiu Lin</a><sup><font color="#A9A9A9">1</font></sup>,
      <a href="https://www.linkedin.com/in/elvishelvisshi/">Jia Shi</a><sup><font color="#A9A9A9">1</font></sup>,
      <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak*</a><sup><font color="#A9A9A9">1</font></sup>,
      <a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan*</a><sup><font color="#A9A9A9">1,2</font></sup>

    </div>

    <div class="institution">
      <sup>1</sup>Carnegie Mellon University 
      <sup>2</sup>Argo AI
    </div>

    <div class="abstract section">
      <h2>Overview</h2>
      <div style="text-align: center">
      <img src="img/examples.png" alt="examples" style="width:800px;height:400px;">
      </div>
      <p style="text-align: justify">
        Continual learning (CL) is widely regarded as crucial challenge for lifelong AI.
        However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make
        use of artificial temporal variation and do not align with or generalize to the realworld. In this paper, we introduce CLEAR, the first continual image classification
        benchmark dataset with a natural temporal evolution of visual concepts in the
        real world that spans a <i> decade</i> (2004-2014). We build CLEAR from existing
        large-scale image collections (YFCC100M) through a novel and scalable low-cost
        approach to visio-linguistic dataset curation. Our pipeline makes use of pretrained
        vision-language models (e.g. CLIP) to interactively build labeled datasets, which
        are further validated with crowd-sourcing to remove errors and even inappropriate
        images (hidden in original YFCC100M). The major strength of CLEAR over
        prior CL benchmarks is the smooth temporal evolution of visual concepts with
        real-world imagery, including both high-quality labeled data along with abundant
        unlabeled samples per time period for continual semi-supervised learning. We
        find that a simple unsupervised pre-training step can already boost state-of-the-art
        CL algorithms that only utilize fully-supervised data. Our analysis also reveals
        that mainstream CL evaluation protocols that train and test on iid data artificially
        inflate performance of CL system. To address this, we propose novel "streaming"
        protocols for CL that always test on the (near) future. Interestingly, streaming
        protocols (a) can simplify dataset curation since today’s testset can be repurposed
        for tomorrow’s trainset and (b) can produce more generalizable models with more
        accurate estimates of performance since all labeled data from each time-period is
        used for both training and testing (unlike classic iid train-test splits).
      </p>
    </div>


    <div class="links section">
<!--      <div class="preview">-->
<!--        <a href="">-->
<!--          <img src="preview/000.png" alt="page 1">-->
<!--          <img src="preview/001.png" alt="page 2">-->
<!--          <img src="preview/002.png" alt="page 3">-->
<!--          <img src="preview/003.png" alt="page 4">-->
<!--          <img src="preview/004.png" alt="page 5">-->
<!--          <img src="preview/005.png" alt="page 6">-->
<!--          <img src="preview/006.png" alt="page 7">-->
<!--          <img src="preview/007.png" alt="page 8">-->
<!--          <img src="preview/011.png" alt="page 12">-->
<!--          <img src="preview/012.png" alt="page 13">-->
<!--          <img src="preview/013.png" alt="page 14">-->
<!--          <img src="preview/014.png" alt="page 15">-->
<!--        </a>-->
<!--      </div>-->
      <h2>Links</h2>
      <ul>
        <li><a href="https://arxiv.org/abs/2201.06289"><strong>Link to our paper</strong></a>: (2021 NeurIPS Datasets and Benchmarks Track accepted)</li>
        
        <li><a href="https://drive.google.com/drive/folders/1subfTCchBR8BWIAoxo7-0k2QgdYjUFUS?usp=sharing"><strong>CLEAR10: Download labeled & unlabeled data & metadata 
              (labeled + unlabeled portion)</strong></a>: For downloading both the labeled and unlabled data, or 7.8M YFCC100M image metadata sorted into 11 temporal buckets (including URL for image download, upload timestamp, user hashtags, etc.).</li>
        <li><a href="https://github.com/linzhiqiu/continual-learning/"><strong>Github repository for YFCC100M download and CLIP-based image retrieval</strong></a>: For downloading YFCC100M dataset and performing CLIP-based image retrieval on YFCC100M.</li>
        <li><a href="https://github.com/ElvishElvis/Continual-Learning"><strong>Github repository for training and evaluation code in our paper</strong></a>: For all experiments in our paper.</li>
        <li><strong>CLEAR100</strong><span></span>: Coming soon on CVPR 2022 workshop challenge</li>
<!--         <li><a href="https://avalanche.continualai.org/">Avalanche-based Evaluation Platform</a><span>(We will host our dataset on Avalanche upon paper acceptance)</span>: For future CL researchers to quickly access CLEAR and perform experiments for reproducibility purposes.</li>
        <li><a href="https://eval.ai">EvalAI Competition Leaderboard</a><span>(Coming soon, we will host our dataset on EvalAI upon paper
            acceptance)</span>: We are planning to host a competition leaderboard on EvalAI upon paper acceptance. Potential challenge tracks include CLEAR-fully-supervised, CLEAR-unsupervised-pretraining, CLIP-continual-semisupervised, and etc.</li> -->
        <!-- <li>Poster <span></span></li> -->
      </ul>
    </div>

    <div class="CLEAR Collection Pipeline">
      <h2>CLEAR Collection Pipeline </h2>
      <div style="text-align: center">
        <img src="img/zhiqiu_clip.png" height="200px" style="width:770px;height:200;margin-right: 46px">
        <img src="img/streaming_unsupervised.png" height="100px" style="width:450px;height:50;margin-right: 46px">
        <img src="img/zhiqiu_streaming.png" height="100px" style="width:450px;height:50;margin-right: 46px">
      </div>
      <p>
        Since it is non-trivial to download the entire YFCC100M dataset (over 10TB), we first download the metadata files (50GB) and download a random subset of 7.8M images from it. Then we use the upload time to recreate the temporal stream and split them into 11 segments of 0.7M images. Given a temporal segment of images and a list of queries (found by text-prompt engineering in order to effectively retrieve the visual concepts of interest), we use a pre-trained vision-language model CLIP to extract their respective L2-normalized image and query features. We can measure how much an image is aligned to a particular query by computing the cosine similiarity score (e.g., dot product) between the two features. For each query, we can rank all the images and only retrieve the ones with highest scores. Finally, we hire MTurk workers to verify the retrieved images for each class and remove sensitive images hidden in original YFCC100M collection.
      </p>
      
    <br>
    </div>
    <div class="class tree">
      <h2>Labeled Data Hierarchy</h2>
      <p>The tree hierarchy of the data is showing as the following</p>

<pre>
<code>root/
|   class_names.txt (each line is a class name)
|   labeled_metadata.json (with paths to labeled images' metadata)
|   all_metadata.json (with paths to all images' metadata)
|   download_all_images.py (for downloading unlabeled images)
└───labeled_images
|   └───1
|   |   └───computer
|   |   |   |   235821044.jpg
|   |   |   |   ...
|   |   └───camera
|   |   |   |   269400202.jpg
|   |   |   |   ...
|   |   └───...
|   └───2
|   └───...
└───labeled_metadata
|   └───1
|   |   |   computer.json (dict: key is flickr ID, value is metadata)
|   |   |   camera.json
|   |   |   ...
|   └───2
|   └───...
└───all_metadata
│   │   0.json
|   |   1.json
|   |   ...
└───features
|   └───moco_b0
|   |   |   features.json (with paths to labeled images' features files)
|   |   |   state_dict.pth.tar (a copy of the state_dict file)
|   |   └───1
|   |   |   |   computer.pth (dict: key is flickr ID, value is torch tensor)
|   |   |   |   camera.pth
|   |   |   |   ...
|   |   └───2
|   |   |   |   ...
|   |   └───...
|   └───moco_imagenet
|   |   └───...
|   └───byol_imagenet
|   |   └───...
|   └───imagenet
|   |   └───...
</code></pre>
    </div>

    <br>
    <div class="Workshop and Competition Information">
      <h2>Workshop and Competition Information</h2>
      <p>
        CLEAR is going to be presented on <a href="http://www.cs.cmu.edu/~shuk/vplow.html"><strong> Visual Perception and Learning in an Open World workshop </strong></a><b>(CVPR2022)</b> and will be included as a challenge track.</p>
    </div>

    <div class="citing section">
      <h2>Citing this work</h2>
      <p>If you find this work useful in your research, please consider citing:</p>
      <pre>@inproceedings{lin2021clear,
          title={The CLEAR Benchmark: Continual LEArning on Real-World Imagery},
          author={Lin, Zhiqiu and Shi, Jia and Pathak, Deepak and Ramanan, Deva},
          booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
          year={2021}}
      </pre>
    </div>
    <br>
    <div class="License">
      <h2>License</h2>
      This project is under
      <a href="https://creativecommons.org/licenses/by/4.0/"><strong>CC BY</strong></a>
      license
    </div>

    <div class="acknowledgements section">
      <h2>Acknowledgements</h2>
      <p>This research was supported by <a href="https://labs.ri.cmu.edu/argo-ai-center/"><strong>CMU Argo AI Center for Autonomous Vehicle Research.</strong></a></p>
    </div>


      

  </div>
</div>
</body>
</html>
