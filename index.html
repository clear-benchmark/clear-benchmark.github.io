
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="keywords" content="computer vision, machine learning, ML datasets and benchmark, continual learning, image recognition, online continual learning"/>
  <title>The CLEAR Benchmark: Continual LEArning on Real-World Imagery
</title>

  <link href="css/normalize.css" rel="stylesheet" type="text/css"/>
  <link href="css/reset.css" rel="stylesheet" type="text/css"/>
  <link href="css/style.css" rel="stylesheet" type="text/css"/>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

</head>
<body>
<div class="container">
  <div class="project-page">

    <div class="affiliation">
      <a href="https://www.ri.cmu.edu/"><img class="leftalign" src="img/cmu_03.png"></a>
    </div>

    <hr>
    <h1>
      <i>The CLEAR Benchmark:</i><br> Continual LEArning on Real-World Imagery</br>
    </h1>

    <hr>

    <div class="abstract section">
      <h2>Overview</h2>
      <div style="text-align: center">
      <img src="img/examples.png" alt="examples" style="width:800px;height:400px;">
      </div>
      <p>
        Existing continual learning (CL) benchmarks, such as Permuted-MNIST and Split-CIFAR, are artificially made and do not generalize to real-world CL applications. We propose CLEAR, the first continual image recognition benchmark dataset with a natural temporal evolution of visual concepts. We argue that temporally-evolving visual concepts offer a more realistic CL scenario with smooth transitions between distributions. Our contributions are twofold: (1) To build CLEAR out of the largest existing public image collection (YFCC100M), we introduce a new low-cost “visio-linguistic-based” dataset curation method by making use of recent zero-shot vision-language models (OpenAI’s CLIP) to dramatically decrease annotation cost, allowing us to create real-world CL datasets that span a decade (2) CLEAR is further post-processed by crowd-sourced validation (Amazon MTurk) to remove misclassified and inappropriate images originally hidden in YFCC100M image collection. The major strengths of CLEAR over prior CL benchmarks include (1) smooth temporal evolution of visual concepts in CLEAR enables a novel and more practical “online” evaluation protocol for continual learning algorithms, which is to “train now and test in the future” (2) CLEAR contains complex real-world imagery (not object-centered, e.g., CIFAR and ImageNet) and is more relevant to real-world applications (3) besides high-quality labeled data, CLEAR offers abundant unlabeled samples for each time period for continual semi-supervised and unsupervised learning.
      </p>
    </div>

    <div class="links section">
<!--      <div class="preview">-->
<!--        <a href="">-->
<!--          <img src="preview/000.png" alt="page 1">-->
<!--          <img src="preview/001.png" alt="page 2">-->
<!--          <img src="preview/002.png" alt="page 3">-->
<!--          <img src="preview/003.png" alt="page 4">-->
<!--          <img src="preview/004.png" alt="page 5">-->
<!--          <img src="preview/005.png" alt="page 6">-->
<!--          <img src="preview/006.png" alt="page 7">-->
<!--          <img src="preview/007.png" alt="page 8">-->
<!--          <img src="preview/011.png" alt="page 12">-->
<!--          <img src="preview/012.png" alt="page 13">-->
<!--          <img src="preview/013.png" alt="page 14">-->
<!--          <img src="preview/014.png" alt="page 15">-->
<!--        </a>-->
<!--      </div>-->
      <h2>Links</h2>
      <ul>
        <li><a href="https://drive.google.com/file/d/1xj5DCYr502SlvoF0INCL1n-gbxi0Kr3v/view?usp=sharing"><strong>Download data</strong></a></li>
        <li>Training code <span>(Coming soon)</span></li>
        <li>Poster <span>(Coming soon)</span></li>
      </ul>
    </div>

    <div class="Collection Pipeline">
      <h2>Collection Pipeline </h2>
      <div style="text-align: center">
        <img src="img/flowchart.png" height="300px" style="width:600px;height:350px;margin-right: 46px">
      </div>
      <p>
        Since it is non-trivial to download the entire YFCC100M dataset (over 10TB), we first download the metadata files (50GB) and download a random subset of 7.8M images from it. Then we use the upload time to recreate the temporal stream and split them into 11 segments of 0.7M images. Given a temporal segment of images and a list of queries (found by text-prompt engineering in order to effectively retrieve the visual concepts of interest), we use a pre-trained vision-language model CLIP to extract their respective L2-normalized image and query features. We can measure how much an image is aligned to a particular query by computing the cosine similiarity score (e.g., dot product) between the two features. For each query, we can rank all the images and only retrieve the ones with highest scores. Finally, we hire MTurk workers to verify the retrieved images for each class and remove sensitive images hidden in original YFCC100M collection.
      </p>
      
    <br>
    </div>
    <div class=" hierarchy">
      <h2>Data Hierarchy</h2>
      <p>The tree hierarchy of the data is showing as the following, click on the arrow(s) to open or close the tree branches.</p>
      <br>
      <ul id="myUL">
        <li> -- query_dict.pickle</li>
        <li><span class="caret">images</span>
          <ul class="nested">
            <li><span class="caret">buckets(X10)</span>
            <ul class="nested">
              <li><span class="caret">classes(X11)</span>
              <ul class="nested">
              <li>1.png</li>
              <li>2.png</li>
              <li>...</li>
            </ul>
          </li>
            </ul>
          </li>
          </ul>
        </li>
      </ul>

      <script>
      var toggler = document.getElementsByClassName("caret");
      var i;

      for (i = 0; i < toggler.length; i++) {
        toggler[i].addEventListener("click", function() {
          this.parentElement.querySelector(".nested").classList.toggle("active");
          this.classList.toggle("caret-down");
        });
      }
      </script>
      <!-- <pre>.
            ├── query_dict.pickle
            │   
            │── images
            │     │
                  ├── bucket_1
                  │   ├── baseball
                  │   │   │
                  │   │   ├── 1.jpg
                  │   │   ├── 2.jpg
                  │   │   └── ....
                  │   │
                  │   ├── bus
                  │   ├── dress
                  │   ├── hockey
                  │   ├── laptop
                  │   ├── racing
                  │   ├── camera
                  │   ├── soccer
                  │   ├── cosplay
                  │   └── NEGATIVE
                  │
                  ├── bucket_2
                  ├── ......
                  └── bucket_10
      </pre> -->
    </div>

    <br>
    <div class="Workshop">
      <h2>Continual LEArning Workshop</h2>
      <p>
        Workshop on continual learning is on the way...
      </p>
    </div>

    <!-- <div class="citing section">
      <h2>Citing this work</h2>
      <p>If you find this work useful in your research, please consider citing:</p>
      <pre>@article{zhao2021camera,
            title={Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias},
            author={Zhao, Yunhan and Kong, Shu and Fowlkes, Charless},
            journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
            year={2021}
            }
      </pre>
    </div> -->
    <br>
    <div class="License">
      <h2>License</h2>
      This project is under
      <a href="https://creativecommons.org/licenses/by/4.0/"><strong>CC BY</strong></a>
      license
    </div>

    <div class="acknowledgements section">
      <h2>Acknowledgements</h2>
      <p>This research was supported by <a href="https://www.ri.cmu.edu/"><strong>CMU Robotic Institute.</strong></a></p>
    </div>


      

  </div>
</div>
</body>
</html>
