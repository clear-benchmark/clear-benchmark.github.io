
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="keywords" content="computer vision, machine learning, ML datasets and benchmark, continual learning, image recognition, online continual learning"/>
  <title>The CLEAR Benchmark: Continual LEArning on Real-World Imagery
</title>

  <link href="css/normalize.css" rel="stylesheet" type="text/css"/>
  <link href="css/reset.css" rel="stylesheet" type="text/css"/>
  <link href="css/style.css" rel="stylesheet" type="text/css"/>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

</head>
<body>
<div class="container">
  <div class="project-page">

    <div class="affiliation">
      <a href="https://www.ri.cmu.edu/"><img class="leftalign" src="img/cmu_03.png"></a>
    </div>

    <hr>
    <h1>
      <i>The CLEAR Benchmark:</i><br> Continual LEArning on Real-World Imagery</br>
    </h1>

    <hr>

    <div class="abstract section">
      <h2>Overview</h2>
      <div style="text-align: center">
      <img src="img/examples.png" alt="examples" style="width:800px;height:400px;">
      </div>
      <p>
        Continual learning (CL) is considered as one of the next big challenges in AI. However, the existing CL benchmarks, e.g.
        Permuted-MNIST and Split-CIFAR, are artificially designed to be made continual and do not align with or generalize to
        real-world. In this paper, we introduce CLEAR, the first continual image recognition benchmark dataset with a natural
        <i>temporal evolution of visual concepts</i> in the real world that spans a <i>decade</i> (2004-2014). We build
        CLEAR from existing image collection (YFCC100M) by proposing a novel low-cost <i>visio-linguistic dataset curation
        approach</i>. It involves using pretrained vision-language models (e.g. CLIP) to quickly build high-quality labeled
        datasets on a tight budget. Finally, we post-process CLEAR via crowd-sourcing to remove errors and even inappropriate
        images hidden in original YFCC100M. The major strengths of CLEAR over prior CL benchmarks include (1) smooth and
        realistic temporal evolution of visual concepts with real-world imagery, enabling a more practical "online" (i.e., train
        on past, test on future) evaluation protocol (2) high-quality labeled data along with abundant unlabeled samples per
        time period for continual semi-supervised and unsupervised learning. Our extensive experiments reveal that mainstream
        "offline" evaluation protocols, which train and test on iid data, artificially inflate performance of CL systems,
        stressing the need for our "online" protocol since the models we train today will always be tested in future. Moreover,
        we find that state-of-the-arts CL algorithms that only utilize fully-supervised data fall short whereas unsupervised
        pretraining provides significant boost. Lastly, we introduce a biased reservoir-sampling algorithm that dynamically
        caches more recent training data, achieving the new state-of-the-arts while still leaving large room for improvement.
      </p>
    </div>


    <div class="links section">
<!--      <div class="preview">-->
<!--        <a href="">-->
<!--          <img src="preview/000.png" alt="page 1">-->
<!--          <img src="preview/001.png" alt="page 2">-->
<!--          <img src="preview/002.png" alt="page 3">-->
<!--          <img src="preview/003.png" alt="page 4">-->
<!--          <img src="preview/004.png" alt="page 5">-->
<!--          <img src="preview/005.png" alt="page 6">-->
<!--          <img src="preview/006.png" alt="page 7">-->
<!--          <img src="preview/007.png" alt="page 8">-->
<!--          <img src="preview/011.png" alt="page 12">-->
<!--          <img src="preview/012.png" alt="page 13">-->
<!--          <img src="preview/013.png" alt="page 14">-->
<!--          <img src="preview/014.png" alt="page 15">-->
<!--        </a>-->
<!--      </div>-->
      <h2>Links</h2>
      <ul>
        <li><a href="https://drive.google.com/file/d/1xj5DCYr502SlvoF0INCL1n-gbxi0Kr3v/view?usp=sharing"><strong>Download data (labeled portion size = 1GB)</strong></a>: For downloading the cleaned and labeled portion (only) of CLEAR.</li>
        <li><a href=""><strong>Download 7.8M metadata
              (labeled + unlabeled portion, coming soon)</strong></a>: For downloading 7.8M YFCC100M image metadata sorted into 11 temporal segments (including URL for image download, upload timestamp, user hashtags, etc.).</li>
        <li><a href="https://github.com/linzhiqiu/continual-learning/"><strong>Github repository for YFCC100M download and CLIP-based image retrieval</strong></a>: For downloading YFCC100M dataset and performing CLIP-based image retrieval on YFCC100M.</li>
        <li>Github repository for training and evaluation code in our paper<span>(Coming soon)</span>: For all experiments in our paper.</li>
        <li><a href="https://avalanche.continualai.org/">Avalanche-based Evaluation Platform</a><span>(We will host our dataset on Avalanche upon paper acceptance)</span>: For future CL researchers to quickly access CLEAR and perform experiments for reproducibility purposes.</li>
        <li><a href="https://eval.ai">EvalAI Competition Leaderboard</a><span>(Coming soon, we will host our dataset on EvalAI upon paper
            acceptance)</span>: We are planning to host a competition leaderboard on EvalAI upon paper acceptance. Potential challenge tracks include CLEAR-fully-supervised, CLEAR-unsupervised-pretraining, CLIP-continual-semisupervised, and etc.</li>
        <li>Poster <span>(Coming soon)</span></li>
      </ul>
    </div>

    <div class="CLEAR Collection Pipeline">
      <h2>CLEAR Collection Pipeline </h2>
      <div style="text-align: center">
        <img src="img/flowchart.png" height="300px" style="width:600px;height:350px;margin-right: 46px">
      </div>
      <p>
        Since it is non-trivial to download the entire YFCC100M dataset (over 10TB), we first download the metadata files (50GB) and download a random subset of 7.8M images from it. Then we use the upload time to recreate the temporal stream and split them into 11 segments of 0.7M images. Given a temporal segment of images and a list of queries (found by text-prompt engineering in order to effectively retrieve the visual concepts of interest), we use a pre-trained vision-language model CLIP to extract their respective L2-normalized image and query features. We can measure how much an image is aligned to a particular query by computing the cosine similiarity score (e.g., dot product) between the two features. For each query, we can rank all the images and only retrieve the ones with highest scores. Finally, we hire MTurk workers to verify the retrieved images for each class and remove sensitive images hidden in original YFCC100M collection.
      </p>
      
    <br>
    </div>
    <div class=" hierarchy">
      <h2>Labeled Data Hierarchy</h2>
      <p>The tree hierarchy of the data is showing as the following, click on the arrow(s) to open or close the tree branches.</p>
      <br>
      <ul id="myUL">
        <li> -- query_dict.pickle (all labeled data metadata)</li>
        <li><span class="caret">images</span>
          <ul class="nested">
            <li><span class="caret">buckets(X10)</span>
            <ul class="nested">
              <li><span class="caret">classes(X11)</span>
              <ul class="nested">
                <!-- <li>NEGATIVE(background)</li>
                <li>computer</li> -->
                <li><span class="caret">computer</span>
                  <ul class='nested'>
                    <li>id_1.png</li>
                    <li>id_2.png</li>
                    <li>...</li>
                  </ul>
                </li>
                <li>...</li>
            </ul>
          </li>
            </ul>
          </li>
          </ul>
        </li>
      </ul>

      <script>
      var toggler = document.getElementsByClassName("caret");
      var i;

      for (i = 0; i < toggler.length; i++) {
        toggler[i].addEventListener("click", function() {
          this.parentElement.querySelector(".nested").classList.toggle("active");
          this.classList.toggle("caret-down");
        });
      }
      </script>
      <!-- <pre>.
            ├── query_dict.pickle
            │   
            │── images
            │     │
                  ├── bucket_1
                  │   ├── baseball
                  │   │   │
                  │   │   ├── 1.jpg
                  │   │   ├── 2.jpg
                  │   │   └── ....
                  │   │
                  │   ├── bus
                  │   ├── dress
                  │   ├── hockey
                  │   ├── laptop
                  │   ├── racing
                  │   ├── camera
                  │   ├── soccer
                  │   ├── cosplay
                  │   └── NEGATIVE
                  │
                  ├── bucket_2
                  ├── ......
                  └── bucket_10
      </pre> -->
    </div>

    <br>
    <div class="Workshop and Competition Information">
      <h2>Workshop and Competition Information</h2>
      <p>
        If CLEAR is going to be presented on any CL/vision workshop or will be included as a challenge track, we will attach the information on this website.
      </p>
    </div>

    <!-- <div class="citing section">
      <h2>Citing this work</h2>
      <p>If you find this work useful in your research, please consider citing:</p>
      <pre>@article{zhao2021camera,
            title={Camera Pose Matters: Improving Depth Prediction by Mitigating Pose Distribution Bias},
            author={Zhao, Yunhan and Kong, Shu and Fowlkes, Charless},
            journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
            year={2021}
            }
      </pre>
    </div> -->
    <br>
    <div class="License">
      <h2>License</h2>
      This project is under
      <a href="https://creativecommons.org/licenses/by/4.0/"><strong>CC BY</strong></a>
      license
    </div>

    <div class="acknowledgements section">
      <h2>Acknowledgements</h2>
      <p>This research was supported by <a href="https://www.ri.cmu.edu/"><strong>CMU Robotic Institute.</strong></a></p>
    </div>


      

  </div>
</div>
</body>
</html>
